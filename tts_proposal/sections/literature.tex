\section*{Literature Review}

Recent advancements in text-to-speech (TTS) synthesis have led to the creation of remarkably high-fidelity models capable of generating human-like speech from various inputs, including short audio prompts \cite{shen2018naturalttssynthesisconditioning,ren2022fastspeech2fasthighquality,kim2020glowttsgenerativeflowtexttospeech,kim2021conditionalvariationalautoencoderadversarial}. This progress, largely driven by neural network architectures, has significant implications for various applications, from virtual assistants and chatbots to audio dramas and interactive content creation \cite{hu2022neuraldubberdubbingvideos,liu2024m3tts}. However, current systems often face challenges when generalizing to diverse languages, particularly in low-resource settings like Bengali and Chinese, and in conveying nuanced emotions \cite{chen2024f5ttsfairytalerfakesfluent, eskimez2024e2ttsembarrassinglyeasy}. \newline


Autoregressive (AR) models, a prevalent approach in TTS, have achieved impressive zero-shot performance. Examples like NaturalSpeech 3 \cite{ju2024naturalspeech3zeroshotspeech} and VALL-E 2 \cite{chen2024valle2neuralcodec} showcase the capability to synthesize diverse speech styles. Yet, AR models often grapple with inference latency, exposure bias issues, and the need for meticulous tokenizer design \cite{song2024ellavstableneuralcodec, du2024valltdecoderonlygenerativetransducer, han2024vallerrobustefficient, peng2024voicecraftzeroshotspeechediting}. This is where non-autoregressive (NAR) methods, employing parallel processing, offer a compelling alternative. \newline

Diffusion models \cite{ho2020denoisingdiffusionprobabilisticmodels}, particularly those utilizing Flow Matching with Optimal Transport (FM-OT) \cite{kornilov2024optimalflowmatchinglearning}, have proven highly effective in NAR TTS systems. These models, exemplified by recent works such as Voicebox \cite{le2023voiceboxtextguidedmultilingualuniversal} and Matcha-TTS \cite{mehta2024matchattsfastttsarchitecture}, directly model the continuous space of audio features, often without explicit phoneme or duration prediction. However, accurately aligning input text to the output synthesized speech remains a significant challenge in NAR models, particularly when dealing with the substantial length differences inherent in these approaches \cite{ju2024naturalspeech3zeroshotspeech}. While frame-wise phoneme alignments have been used in some models, recent research indicates that these can be less effective for naturalness. Methods that skip explicit phoneme-level duration modeling, like E2 TTS \cite{eskimez2024e2ttsembarrassinglyeasy} and Seed-TTS \cite{anastassiou2024seedttsfamilyhighqualityversatile}, frequently demonstrate more natural prosody. These models often rely on the model implicitly inferring the duration from the overall sequence length during inference. \newline

The need for robustness in text-speech alignment is highlighted in the literature, especially concerning multilingual TTS \cite{saeki2024extendingmultilingualspeechsynthesis}. The issue of data scarcity, prevalent in under-resourced languages like Bengali and Chinese, necessitates innovative approaches for model training. Data augmentation and enhancement techniques can help in these cases. Models like DiTTo-TTS \cite{lee2024dittottsefficientscalablezeroshot} attempt to integrate semantic information via pre-trained language models. However, the most effective approach to handle this need for alignment and efficient synthesis, especially for multilingual settings, is still a topic of ongoing research. The research proposed in this paper, F5-TTS \cite{chen2024f5ttsfairytalerfakesfluent}, seeks to build upon this recent work by focusing on a simpler approach that avoids explicit phoneme-based duration models while achieving comparable, or perhaps superior, performance, especially for robustness. \newline

In conclusion, the field of TTS is evolving rapidly, with a shift toward more efficient and flexible NAR models. However, robust text-speech alignment and addressing the challenges of under-resourced languages remain critical areas for further exploration. This research aims to contribute to this field by developing a robust, multilingual TTS framework for English, Bengali, and Chinese that balances synthesis quality with efficiency, especially when considering industry-level application needs.
