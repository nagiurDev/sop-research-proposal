\section*{Introduction}

Action recognition, the automated identification of human actions in video, is a vital component of modern computer vision, impacting diverse applications like surveillance, human-computer interaction, and robotics. Its ability to interpret human behavior is fundamental for creating intelligent systems capable of interacting seamlessly with the world. A significant hurdle, however, lies in the substantial amount of labeled data traditionally required to train robust action recognition models. This poses a particular challenge for few-shot action recognition, where the goal is to learn new actions from only a limited number of examples. Standard deep learning models, optimized on massive datasets, often struggle to generalize effectively in these data-constrained scenarios, tending to overfit the available training data and exhibiting poor performance on unseen instances.\newline

Existing approaches to few-shot action recognition explore meta-learning and transfer learning techniques. Meta-learning aims to learn how to learn, enabling models to quickly adapt to new tasks. Transfer learning leverages knowledge gained from pre-trained models on large datasets. While promising, these methods have limitations. A common technique to boost performance is data augmentation, which artificially expands the training data. Traditional data augmentation relies on random transformations such as cropping, rotation, and jittering. However, these transformations may not generate semantically relevant variations of the action, limiting their effectiveness in the few-shot setting, where a nuanced understanding is crucial.\newline

Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language. Their potential to enhance computer vision, especially in guiding data augmentation, remains largely unexplored. We propose a novel approach leveraging LLMs to guide data augmentation for few-shot action recognition. We hypothesize that by using LLMs to generate descriptive and even counterfactual variations of actions (e.g., "what if the person was also holding a ball?"), we can create more targeted and effective synthetic training examples. This LLM-guided augmentation ensures that the generated variations are diverse and semantically grounded, leading to improved generalization. Our research aims to develop and evaluate this framework, investigating different LLM architectures, prompting strategies, and augmentation techniques to optimize synthetic data generation and improve few-shot action recognition performance on benchmark datasets.\newline