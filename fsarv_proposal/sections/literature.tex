\section*{Literature Review}

Few-shot action recognition aims to enable models to recognize novel actions from limited labeled examples. This challenging task has garnered significant attention, with various approaches being explored, including meta-learning, transfer learning, and data augmentation. \newline

Meta-learning algorithms, such as Model-Agnostic Meta-Learning (MAML) \cite{finn2017model} and TinyReptile \cite{ren2023tinyreptile}, aim to learn a model initialization that facilitates rapid adaptation to new tasks with minimal data. These methods have shown promise in few-shot image classification \cite{chen2024few} and have been adapted for action recognition. Transfer learning approaches leverage pre-trained models on large-scale datasets like Kinetics \cite{kay2017kinetics} to provide a strong starting point for few-shot learning \cite{brown2020language}. However, the domain gap between source and target tasks can still hinder performance. Metric learning methods, like Relation Networks \cite{sung2018learning}, learn an embedding space where similar actions are closer together, enabling effective comparison of few-shot examples.\newline

Data augmentation is a crucial technique for mitigating the impact of limited data. Traditional methods involve applying transformations like cropping, rotation, flipping, and jittering \cite{cai2021jitter}. More advanced techniques like Mixup \cite{zhang2017mixup} and CutMix \cite{yun2019cutmix} generate synthetic samples by blending or splicing images/videos, improving model robustness. However, these methods often apply transformations randomly, lacking semantic understanding of the actions. This can lead to unrealistic or irrelevant variations, which may not be beneficial, especially in the few-shot setting where preserving the core characteristics of the limited examples is crucial. Recent work has explored using GANs \cite{goodfellow2014generative} and diffusion models \cite{chen2024overview} for data augmentation, but these methods can be computationally expensive and challenging to train, particularly for complex data like videos.\newline

LLMs like GPT-3 \cite{radford2021learning} and BERT \cite{devlin2018bert} have demonstrated remarkable capabilities in understanding and generating human language. These models have been successfully applied in various NLP tasks, but their potential in computer vision remains largely untapped. Vision-language models like CLIP \cite{radford2021learning} bridge the gap between visual and textual information, enabling zero-shot classification and other cross-modal tasks. This suggests the possibility of leveraging LLMs to generate descriptive variations of actions, which can then guide the data augmentation process.\newline

Despite the advancements in few-shot learning, data augmentation, and LLMs, there remains a significant gap in effectively utilizing semantic information to guide data augmentation for few-shot action recognition. Existing methods either rely on random transformations or require computationally intensive generative models. There is a need for a more efficient and semantically-aware approach to data augmentation that can improve the generalization capabilities of few-shot action recognition models.\newline

This research proposes a novel framework that leverages the power of LLMs to guide data augmentation for few-shot action recognition. By using LLMs to generate descriptive variations of actions, we aim to create more targeted and meaningful synthetic training examples. This approach seeks to address the limitations of existing data augmentation techniques by incorporating semantic understanding into the augmentation process, ultimately leading to improved performance in few-shot scenarios.